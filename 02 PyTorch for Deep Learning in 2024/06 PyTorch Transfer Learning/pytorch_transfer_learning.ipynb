{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "What is transfer learning?\n",
        "\n",
        "Transfer learning involves taking the parameters of what one model has learn in another dataset and applying them to our own problem.\n",
        "\n",
        "* Pretrained model = function models"
      ],
      "metadata": {
        "id": "_67UMdmA_5AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__) # want 1.12+\n",
        "print(torchvision.__version__) # want 0.13+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhkZCfwV8Wvy",
        "outputId": "18eb722e-7501-429d-c4cb-42f6be4a1612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "0.16.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with upadated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "  import torch\n",
        "  import torchvision\n",
        "  assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "  assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "  print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "  !pip3 install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "  import torch\n",
        "  import torchvision\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "wedEUFQ48kvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got the versions of torch and torchvision, we're after, let's import the code we've written in pervious sections so that we don't have to write it all again."
      ],
      "metadata": {
        "id": "OB4LyR4KUgZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "oa0Bfymk-Q-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936b762f-ae20-4dd7-d63d-734c49b29f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Counting objects: 100% (1234/1234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 4056 (delta 1141), reused 1124 (delta 1124), pack-reused 2822\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 649.94 MiB | 16.07 MiB/s, done.\n",
            "Resolving deltas: 100% (2386/2386), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cjYFQIydWzo9",
        "outputId": "b7db8bd3-409b-41af-9154-863dea9fb8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data\n",
        "\n",
        "We need out pizza, steak, sushi data to build a transfer learning model on."
      ],
      "metadata": {
        "id": "38xEAj2BYQde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Setuo data path\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\" # images from a subset of classes from the Food101 dataset\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists, skipping re-download.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path}, downloading it...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Download pizza, steak, sushi data\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  # Unzip pizza, steak, sushi data\n",
        "  with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  # Remove .zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQDkX-PaX8C1",
        "outputId": "7f6c4f02-c616-42ca-c903-7daafab117bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi, downloading it...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory path\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "i5g0vfWGbxG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86d4d60-53fc-42d7-e86b-e4de2f2d67f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoader\n",
        "\n",
        "Now we've got some data, want to turn it into PyTorch DataLoaders.\n",
        "\n",
        "To do so, we can use `data_setup.py` and the `create_dataloaders()` function we made in 05.PyTorch Going Modular.\n",
        "\n",
        "There's one thing we have to think about when loading: how **transform** it?\n",
        "\n",
        "And with `torchvision` 0.13+ there's two ways to do this:\n",
        "\n",
        "1. Manualy created transform - you define what transform you want your data to go through.\n",
        "2. Automatically created transform - the transforms for your data are defined by the model you'd like to use.\n",
        "\n",
        "`Important point`: when using a pretrained model, it's important that the data (including your custom data) that you pass through it is **transformed** in the same way that the data the model was trained on."
      ],
      "metadata": {
        "id": "wNQFYtNY1hMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup"
      ],
      "metadata": {
        "id": "z9qHy_5ydfQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "`torchvision.models` contains pretrained models (models ready for transfer learning) right within `torchvision`.\n",
        "\n",
        "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean=[0.485, 0.456, 0.406] ad std=[0.229, 0.224, 0.225]. You can use the following transform to normalize."
      ],
      "metadata": {
        "id": "Q-I_nSk93bhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)), # resize image to 224, 224 (height, width)\n",
        "    transforms.ToTensor(), # get images into range [0, 1]\n",
        "    normalize # make sure images have the same distribution as ImageNet (where our pretrained models have been trained)\n",
        "])"
      ],
      "metadata": {
        "id": "TV-lfup-3zem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms,\n",
        "                                                                               batch_size=32)\n",
        "\n",
        "train_data"
      ],
      "metadata": {
        "id": "SRdv1nZF59qZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}